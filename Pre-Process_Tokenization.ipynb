{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Extract tokens/terms/words from sentences\n",
    "- Tokenizers from packages like NLTK (Natural Language Toolkit) & spaCY to extract tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "8QXQuiNwb9Tr"
   },
   "outputs": [],
   "source": [
    "import pandas as pd, spacy, nltk, re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5562,
     "status": "ok",
     "timestamp": 1611042924173,
     "user": {
      "displayName": "Varun Rana",
      "photoUrl": "",
      "userId": "00188788562004174296"
     },
     "user_tz": -330
    },
    "id": "fEirLWgDzQm5",
    "outputId": "ec61472b-a9d5-439f-eaaa-5debb835d59f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/sylvia/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One document will always be in form of 1 string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "8JDiiFTZ3psb"
   },
   "outputs": [],
   "source": [
    "doc = 'I visited my grandparents last week; We had a good time together'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to extract words there are 2 ways - you could do it manually or u can use library like nltk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V_6hHMe8M5II"
   },
   "source": [
    "# Manual Process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5548,
     "status": "ok",
     "timestamp": 1611042924176,
     "user": {
      "displayName": "Varun Rana",
      "photoUrl": "",
      "userId": "00188788562004174296"
     },
     "user_tz": -330
    },
    "id": "UZJeKzCJ351Q",
    "outputId": "47e3985a-e98b-477e-eb8d-b918e063722f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'visited',\n",
       " 'my',\n",
       " 'grandparents',\n",
       " 'last',\n",
       " 'week;',\n",
       " 'we',\n",
       " 'had',\n",
       " 'a',\n",
       " 'good',\n",
       " 'time',\n",
       " 'together']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use lower and split the doc\n",
    "tokens = doc.lower().split()\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 5534,
     "status": "ok",
     "timestamp": 1611042924178,
     "user": {
      "displayName": "Varun Rana",
      "photoUrl": "",
      "userId": "00188788562004174296"
     },
     "user_tz": -330
    },
    "id": "EJWxRoGi4Fbk",
    "outputId": "a90aeb72-aed4-4581-c5bc-ad462d1ad3ab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i visited my grandparents last week we had a good time together'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove characters - replacing not a word character with space\n",
    "doc_cleaned = re.sub('[^\\w\\s]','',doc.lower()) # removing semi-colon\n",
    "doc_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5523,
     "status": "ok",
     "timestamp": 1611042924179,
     "user": {
      "displayName": "Varun Rana",
      "photoUrl": "",
      "userId": "00188788562004174296"
     },
     "user_tz": -330
    },
    "id": "OPKs5sIL4nU0",
    "outputId": "7f314791-734d-400d-8b4e-3cfb5f42e48d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'visited',\n",
       " 'my',\n",
       " 'grandparents',\n",
       " 'last',\n",
       " 'week',\n",
       " 'we',\n",
       " 'had',\n",
       " 'a',\n",
       " 'good',\n",
       " 'time',\n",
       " 'together']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split the cleaned doc using space \n",
    "tokens = doc_cleaned.split(' ')\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BOYF76sxNK7O"
   },
   "source": [
    "# Automated process of tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 major types of tokenizer are - word_tokenize, RegexpTokenizer and TweetTokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5516,
     "status": "ok",
     "timestamp": 1611042924182,
     "user": {
      "displayName": "Varun Rana",
      "photoUrl": "",
      "userId": "00188788562004174296"
     },
     "user_tz": -330
    },
    "id": "tWAuh6FrMmtL",
    "outputId": "c679c943-5063-4662-c3b7-dc7e0a7f5e25"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'visited',\n",
       " 'my',\n",
       " 'grandparents',\n",
       " 'last',\n",
       " 'week',\n",
       " ';',\n",
       " 'we',\n",
       " 'had',\n",
       " 'a',\n",
       " 'good',\n",
       " 'time',\n",
       " 'together']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(doc.lower())\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dxd9PFw1Oyvx"
   },
   "source": [
    "In the above list ';' came as a seperate token compared to manual process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wNAFzitqPROe"
   },
   "source": [
    "# **RegexpTokenizer** - If we want to apply regular expressions and then want to extract the token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5508,
     "status": "ok",
     "timestamp": 1611042924184,
     "user": {
      "displayName": "Varun Rana",
      "photoUrl": "",
      "userId": "00188788562004174296"
     },
     "user_tz": -330
    },
    "id": "mtZqAlmrNIlh",
    "outputId": "b2e62675-cafa-4aec-a965-a7b33be33017"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'visited',\n",
       " 'my',\n",
       " 'grandparents',\n",
       " 'last',\n",
       " 'week',\n",
       " 'we',\n",
       " 'had',\n",
       " 'a',\n",
       " 'good',\n",
       " 'time',\n",
       " 'together']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+') # setting the pattern to make tokens of only sequence of words and nothing else\n",
    "tokens = tokenizer.tokenize(doc.lower()) \n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "xPP3RT3pQfpJ"
   },
   "outputs": [],
   "source": [
    "doc2 = '@john This product is really cool!!!üòÄüòÉüòÑüòÅüòÜüòÖ #awesome'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case of tweet messages if you use normal tokenizer i.e. word_tokenize then @, emoji's, !, # will be recognised as seperate tokens which would not be helpful cz we would like @john as username to appear together and #awesome to appear together. For this use TweetTokenizer from nltk library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5496,
     "status": "ok",
     "timestamp": 1611042924187,
     "user": {
      "displayName": "Varun Rana",
      "photoUrl": "",
      "userId": "00188788562004174296"
     },
     "user_tz": -330
    },
    "id": "jQ3WuKTltgth",
    "outputId": "4db83a4b-db57-48b6-e5da-1c4d068e3772"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@',\n",
       " 'john',\n",
       " 'This',\n",
       " 'product',\n",
       " 'is',\n",
       " 'really',\n",
       " 'cool',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " 'üòÄüòÉüòÑüòÅüòÜüòÖ',\n",
       " '#',\n",
       " 'awesome']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = word_tokenize(doc2)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x0_5wq_hvYc5"
   },
   "source": [
    "* @john, #awesome should have come together but\n",
    "* All the tokens except pair of smileys are coming in seperate lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5488,
     "status": "ok",
     "timestamp": 1611042924190,
     "user": {
      "displayName": "Varun Rana",
      "photoUrl": "",
      "userId": "00188788562004174296"
     },
     "user_tz": -330
    },
    "id": "GABop-eAvS8E",
    "outputId": "41fa1097-a6cc-472e-9e28-45aaf67b6940"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@john',\n",
       " 'This',\n",
       " 'product',\n",
       " 'is',\n",
       " 'really',\n",
       " 'cool',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " 'üòÄ',\n",
       " 'üòÉ',\n",
       " 'üòÑ',\n",
       " 'üòÅ',\n",
       " 'üòÜ',\n",
       " 'üòÖ',\n",
       " '#awesome']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "token1 = tweet_tokenizer.tokenize(doc2) # doc here will be 1 tweet message at a time\n",
    "token1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h_hfKPe-w_Ca"
   },
   "source": [
    "* Now @john and #awesome are coming together as they should have and smileys are coming in seperate lines "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0S8OTqKLqrEI"
   },
   "source": [
    "# Tokenization - CSV file read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VY_6uSnTw-Oh"
   },
   "source": [
    "[link text](https://)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "INWvaljvruSw"
   },
   "outputs": [],
   "source": [
    "# !pip install -U -q PyDrive\n",
    "# from pydrive.auth import GoogleAuth\n",
    "# from pydrive.drive import GoogleDrive\n",
    "# from google.colab import auth\n",
    "# from oauth2client.client import GoogleCredentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "hbK_lcdQ0QhW"
   },
   "outputs": [],
   "source": [
    "# auth.authenticate_user()\n",
    "# gauth = GoogleAuth()\n",
    "# gauth.credentials = GoogleCredentials.get_application_default()\n",
    "# drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "-HS-nl1e0VwC"
   },
   "outputs": [],
   "source": [
    "# downloaded = drive.CreateFile({'id':'12CUjW29tTTxYAcPhxuKb_qSn0UTzc4BR'}) # replace the id with id of file you want to access\n",
    "# downloaded.GetContentFile('imdb_sentiment.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/sylvia/Desktop/IITR/M6_text_analytics/Times_Demo_Videos'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract tokens from a text column in a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "executionInfo": {
     "elapsed": 1177,
     "status": "ok",
     "timestamp": 1611047092973,
     "user": {
      "displayName": "Varun Rana",
      "photoUrl": "",
      "userId": "00188788562004174296"
     },
     "user_tz": -330
    },
    "id": "dhrU-olG3-Mv",
    "outputId": "60650d57-45e7-45eb-cdaa-be5c15cbbdc3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A very, very, very slow-moving, aimless movie ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not sure who was more lost - the flat characte...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Attempting artiness with black &amp; white and cle...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Very little music or anything to speak of.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The best scene in the movie was when Gerardo i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  A very, very, very slow-moving, aimless movie ...          0\n",
       "1  Not sure who was more lost - the flat characte...          0\n",
       "2  Attempting artiness with black & white and cle...          0\n",
       "3       Very little music or anything to speak of.            0\n",
       "4  The best scene in the movie was when Gerardo i...          1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('/Users/sylvia/Desktop/datasets/imdb_sentiment.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting tokens from csv/pdf files. Here every row will be considered as one document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1590,
     "status": "ok",
     "timestamp": 1611054177105,
     "user": {
      "displayName": "Varun Rana",
      "photoUrl": "",
      "userId": "00188788562004174296"
     },
     "user_tz": -330
    },
    "id": "q1wbr9NO47tF",
    "outputId": "7a4ddbd9-ecea-4bd6-80af-7961129854e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a very, very, very slow-moving, aimless movie about a distressed, drifting young man.  \n",
      "['a', 'very', 'very', 'very', 'slow', 'moving', 'aimless', 'movie', 'about', 'a', 'distressed', 'drifting', 'young', 'man'] \n",
      "\n",
      "not sure who was more lost - the flat characters or the audience, nearly half of whom walked out.  \n",
      "['not', 'sure', 'who', 'was', 'more', 'lost', 'the', 'flat', 'characters', 'or', 'the', 'audience', 'nearly', 'half', 'of', 'whom', 'walked', 'out'] \n",
      "\n",
      "attempting artiness with black & white and clever camera angles, the movie disappointed - became even more ridiculous - as the acting was poor and the plot and lines almost non-existent.  \n",
      "['attempting', 'artiness', 'with', 'black', 'white', 'and', 'clever', 'camera', 'angles', 'the', 'movie', 'disappointed', 'became', 'even', 'more', 'ridiculous', 'as', 'the', 'acting', 'was', 'poor', 'and', 'the', 'plot', 'and', 'lines', 'almost', 'non', 'existent'] \n",
      "\n",
      "very little music or anything to speak of.  \n",
      "['very', 'little', 'music', 'or', 'anything', 'to', 'speak', 'of'] \n",
      "\n",
      "the best scene in the movie was when gerardo is trying to find a song that keeps running through his head.  \n",
      "['the', 'best', 'scene', 'in', 'the', 'movie', 'was', 'when', 'gerardo', 'is', 'trying', 'to', 'find', 'a', 'song', 'that', 'keeps', 'running', 'through', 'his', 'head'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "docs = data['review'].str.lower()  # access review column & convert it to lowercase\n",
    "tokenizer = RegexpTokenizer('\\w+')  # RegexpTokenizer so can retain only sequence of word characters\n",
    "\n",
    "# looping through first 5 docs in my dataset\n",
    "\n",
    "for x in docs.head():\n",
    "  tokens = tokenizer.tokenize(x)   # tokenizer here is RegexpTokenizer\n",
    "  print(x)\n",
    "  print(tokens,'\\n')\n",
    "  #print('-'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1190,
     "status": "ok",
     "timestamp": 1611054434628,
     "user": {
      "displayName": "Varun Rana",
      "photoUrl": "",
      "userId": "00188788562004174296"
     },
     "user_tz": -330
    },
    "id": "AEUPA2zI78eX",
    "outputId": "7dba80a9-fd1d-4ba2-b4fa-5142c3409fc9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['a',\n",
       "  'very',\n",
       "  'very',\n",
       "  'very',\n",
       "  'slow',\n",
       "  'moving',\n",
       "  'aimless',\n",
       "  'movie',\n",
       "  'about',\n",
       "  'a',\n",
       "  'distressed',\n",
       "  'drifting',\n",
       "  'young',\n",
       "  'man'],\n",
       " ['not',\n",
       "  'sure',\n",
       "  'who',\n",
       "  'was',\n",
       "  'more',\n",
       "  'lost',\n",
       "  'the',\n",
       "  'flat',\n",
       "  'characters',\n",
       "  'or',\n",
       "  'the',\n",
       "  'audience',\n",
       "  'nearly',\n",
       "  'half',\n",
       "  'of',\n",
       "  'whom',\n",
       "  'walked',\n",
       "  'out'],\n",
       " ['attempting',\n",
       "  'artiness',\n",
       "  'with',\n",
       "  'black',\n",
       "  'white',\n",
       "  'and',\n",
       "  'clever',\n",
       "  'camera',\n",
       "  'angles',\n",
       "  'the',\n",
       "  'movie',\n",
       "  'disappointed',\n",
       "  'became',\n",
       "  'even',\n",
       "  'more',\n",
       "  'ridiculous',\n",
       "  'as',\n",
       "  'the',\n",
       "  'acting',\n",
       "  'was',\n",
       "  'poor',\n",
       "  'and',\n",
       "  'the',\n",
       "  'plot',\n",
       "  'and',\n",
       "  'lines',\n",
       "  'almost',\n",
       "  'non',\n",
       "  'existent'],\n",
       " ['very', 'little', 'music', 'or', 'anything', 'to', 'speak', 'of'],\n",
       " ['the',\n",
       "  'best',\n",
       "  'scene',\n",
       "  'in',\n",
       "  'the',\n",
       "  'movie',\n",
       "  'was',\n",
       "  'when',\n",
       "  'gerardo',\n",
       "  'is',\n",
       "  'trying',\n",
       "  'to',\n",
       "  'find',\n",
       "  'a',\n",
       "  'song',\n",
       "  'that',\n",
       "  'keeps',\n",
       "  'running',\n",
       "  'through',\n",
       "  'his',\n",
       "  'head']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = data['review'].str.lower()\n",
    "docs_cleaned = []\n",
    "tokenizer = RegexpTokenizer('\\w+')\n",
    "for x in docs.head():\n",
    "  tokens = tokenizer.tokenize(x)\n",
    "  docs_cleaned.append(tokens)\n",
    "docs_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kwNdnGzoVV3j"
   },
   "source": [
    "* Here we have list of lists which contains list elements in rows for e.g.-> 1st row in docs became 1st list, 2nd row in docs became 2nd list "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "czOOyIveTURm"
   },
   "source": [
    "# Use spacy library to get individual tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "nw_doc = 'I visited my grandparents last week; We had a good time together'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1666,
     "status": "ok",
     "timestamp": 1611055772113,
     "user": {
      "displayName": "Varun Rana",
      "photoUrl": "",
      "userId": "00188788562004174296"
     },
     "user_tz": -330
    },
    "id": "dwLYWOinUsXV",
    "outputId": "7a4199ad-bf77-4d99-a7f0-d769ebfbf00f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i\n",
      "visited\n",
      "my\n",
      "grandparents\n",
      "last\n",
      "week\n",
      ";\n",
      "we\n",
      "had\n",
      "a\n",
      "good\n",
      "time\n",
      "together\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm') # Necessary corpus required to do text cleaning and processing operations \n",
    "\n",
    "spacy_doc = nlp(nw_doc.lower())\n",
    "\n",
    "for x in spacy_doc:\n",
    "  print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "khYhIa-EaPnO"
   },
   "source": [
    "* Here we got individual tokens automatically from a single document "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PUeNWEPBafYD"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMwWems4vTDf0kqXSWWaUPb",
   "collapsed_sections": [],
   "name": "demovid3_Tokenization.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
